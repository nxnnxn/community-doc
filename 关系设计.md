| 版本   | 时间        | 修改人 | 备注  |
|------|-----------|-----|-----|
| V1.0 | 2022.9.22 | 如闻  | 初始化 |


## 1.需求背景

### 1.1.概述

### 1.2.术语

| 术语   | 定义        | 备注  |
|------|-----------|-----|
| V1.0 | 2022.9.22 | 初始化 |

## 2.需求分析

### 2.1.业务分析

### 2.2.技术目标

场景 目标 备注

### 2.3.复杂度分析

#### 2.3.1.高可用分析

1. 系统容量预估:10亿注册用户,平均点赞数500,无上限,平均收藏数100,平均关注数100,未来可能会支持其他类型，所以总数据位900*
   10亿=9000亿,假设向上取10000亿,假设现状我们有800亿条记录.
2. 未来增量数据预估:日活2亿,点赞率按照8%,每人关注2来计算,一天2亿*0.08(点赞率)*2大概3200万条记录,收藏我们大概2亿*0.03*
   1.5(收藏率)=900W,向上整体估算5000W/天，我们预估2年,365*2*5000W=360亿,现状我们有800亿条记录，所以总体按照1160亿估算
3. 对于系统来说,关注的业务价值最高,其他的其次,因此针对这些数据来讲，要保证写入,存储和读取的高可用性,针对于除了关注的系统,
   在读取阶段可以适当的降级，关注系统读取阶段要做到高可用。
4. 需要支持异地多机房,未来可支持单元化。

#### 2.3.2.高性能分析

1. 写入QPS预估:平均QPS=5000W/3600/24=578,峰值计算逻辑是在20%的时间之内有了80%的流量，5000W*0.8/3600/24/0.2=2314,
   写入流量一般比较小,我们预留一定的系统容量应对后续业务的发展和突然来的峰值,我们将设计目标设定为峰值的5倍,2314*
   5=11574QPS
2. 系统读取流量预估,峰值QPS整体按照10倍预估
    1. 帖子FEED流上获取用户是否被点赞/关注/收藏+各种统计数据,2亿*6(
       一天打开6次)*3(平均下查3个请求)/3600/24=41666，峰值=平均*10=41W
    2. 获取用户粉丝数/接口, 整体流量一般个人主页,平均 QPS2亿*6*0.3(转化)*2(用户重复看)/3600/24=8333,峰值QPS=平均*
       10=4.1W.
    3. 获取用户关注用户列表，获取粉丝列表，这些基本都是点击数字后所展现内容，12.5W*10%=12500QPS
    4. 所以系统需要需要41W+12.5W+1.25W=55W,需要对内扩展支撑25W,因此需要80WQPS

#### 2.3.3.可扩展分析

1. 系统要支持未来业务对用户关系的增加,有其通用性,但整体还是一个面对用户的,一个是面对内容的,其他场景需要讨论.
2. 可支持与其他系统做集成,可拆分能力单独服务,如共享系统只需要按照PV在+-;帖子播放次数也是,评论系统可以将数据给到做集成.
   有一些哪怕需要存储detail，但不需要存在性判断,等等设计，因此要类型接入的时候,可以支持自定义.

## 3.概要设计

### 3.1.技术选型

#### 3.1.1.数据库选型

##### 3.1.1.1.主数据库选型

1. 数据库存储了大量的数据,估算1160亿*42/1024/1024/1024=4537G,索引要建立用户到时间,对象到时间,大体基本按照1160亿*
   30/1024/1024/1024*2=6482G,需要11T数据
2. 数据查询维度有记录和统计,如果在mysql上分库分表的话需要记录翻倍,在某些场景上需要存储额外字段用于统计,如点赞需要存储内容的发布者ID,
   用于计算整体发布者的总赞数,因此需要多存储一些信息,因此数据库字段需要可随意添加
3. 因此整体建议表格存储

##### 3.1.1.2.统计数据库选型

1. 统计维度跟随detail数据一起来完成计算,如用户关注统计,用户粉丝统计等,,在业务上每次计算需要对唯一健后进行+-,
   空间计算大概200亿*42/1024/1024/1024=558G
2. 如果用MYSQL存储,需要固定唯一健,若要下层级看个数,需要重新申请新表来重新计算数据.
3. 如果用NOSQL存储,唯一健后,统计维度数据列可任意,若有新,可拓展rowkey来新增,不用新增表结构.
4. 建议NOSQL表格存储. 整体建议选择两套集群,避免过多打扰

#### 3.1.2.缓存选型

##### 3.1.2.1.存在型判断选择

我们查询场景第一为存在型判断,判断A和B的用户关系是否在,redis中string,set,bitmap,布隆过滤器,布谷鸟过滤器等

1. string类型很简单，但我们大体只会存储两个long型，共16字节，但redis object,SDS等数据会占用大量的数据因此不适合
2. set类型,由于用户点赞&收藏无上限,整体会导致会让一些用户成为bigkey.
3. bitmap由于支持数字型，但我们的帖子ID，用户ID数字过于庞大，超过了2的32次方，哪怕我们只有10亿用户，但用户ID基本是不连续,但唯一，因此不适合。
4. 布隆过滤器可以告诉我们 “某样东西一定不存在或者可能存在”，也就是说布隆过滤器说这个数不存在则一定不存，布隆过滤器说这个数存在可能不存在,
   前面场景告诉我们大部分推荐出来的数据是我们没有点赞,没有收藏等的.缺点是布隆过滤器不支持删除，假如我们点赞了，将数据添加到过滤器中,后续我们又取消点赞了，
   我们查的时候他有可能告诉我们去他点赞了，这个时候我们再到库里查一下也是可以的.
5. 布谷鸟过滤器(https://redis.io/docs/data-types/probabilistic/cuckoo-filter/)
   他改进了布隆过滤器不能删除的特性，同时拥有布隆过滤器的能力，但缺点是删除需要替换元素。整体还好，属于redis stack中的能力
6. 整体看下来用布隆过滤器比较靠谱，实际请求中99%的没有动作的,因此41WQPS其实到达数据库大体有4000差不多.

##### 3.1.2.2.统计数据

1. 统计数据需要承担所有统计的流量,有几种数据结构string,第二hash
    1. String类型有两种模式，一种是key区分点赞还是关注,value为long型，我们取数据由于集群版本redis支持mget,
       因此获取多条数据时候多个如点赞数,评论数需要多次MGET。
    2. String的另一种模式是和数据库一样,key就是内容ID或用户ID,value是一个String,若有变化,异步通过消息方式来将数据同步过去
    3. HASH模式,类似于上面,只不过数据已经基本维持在一个HASH表里。
    4. 整体检验缓存结构为String,里边是复合数据类型.
2. 统计数据上有很多内容为0的数据,导致缓存穿透,缓存穿透一般用布隆过滤器
    1. 在我们上面设计场景里,只要一条数据不都是0,那么数据库里就存在这个字段,而以推荐模式下大部数据是热数据,基本都有一些互动数。
    2. 所有先MGET内容,在挑选缓存不存在的数据,有两种可能一种是缓存过期了,一种是数据库里没有,然后到布隆过滤中查看,若有就查库,若无就直接返回。
       整体检验也是两套缓存集群,避免过多打扰

#### 3.1.3.多机房选型

##### 3.1.3.1.数据库多机房策略

1. 在多机房场景,针对于表格存储,现阶段支持多数据中心机制,双向同步关系,但整体空间double,若未来单元化由于当前数据库只保存当前单元用户数据信息,
   未来关闭同步,在业务上也可以接受。

##### 3.1.3.2.缓存redis部署多机房业务使用

1. 针对缓存而言，TAIR提供里基本能力https://help.aliyun.com/zh/tair/user-guide/limits
2. 由于我们在存在性判断上是需要全量同步,在统计数据上不太需要,但即使做了同步也关系不大.

#### 3.1.4.关注是否独立

1. 每块业务都有其独立的特性,都有其迭代的要求,但整个社区系统而言,用户,关注,内容是最重要的,同时在上面可以延展出更多的信息，包含互相关注等等，
   业务上也会和这些关系系统复杂一点。因此建议单独系统,但同时一些能力可以共同使用.

#### 3.1.5.对外系统接入选择

1. 我们的主查询场景是feed流各种调用我们的数据,因此整体流量比较大。大概有41W QPS都来自这里，如果我们4C8G单台支撑1000QPS的话需要400台机器才可以。
2. 面对这么大量且业务稳定的数据，建议使用SDK封装访问缓存和缓存后RPC能力,从而让整个集群降低负载,但复杂度是需要产品化SDK,不能将SDK随意给出,不然容易将缓存系统暴露出去.

### 3.2.容量预估

#### 3.2.1.表格存储容量预估

1. 历史记录容量预估
    1. 存储成本11T每G大该1.08若打5折,一般价格在11900CNY/月，若买容量包，基本在8000左右，
    2. 计算资源按照2WQPS预估,计算可弹性基本价格在3000左右因此单个成本大概在11000左右,
    3. 总体成本11000*3=33000CNY/月
2. 统计数据
    1. 每G大该1.08若打5折,558*1.08*0.5=300,若容量包,基本200块钱
    2. FEED流基本6条,缓存命中率基本在9成左右,41W*0.1=4.1WQPS,因此向上预估5WQPS,5W=7500,正常流量可弹性
    3. 总成本=8000*3=24000CNY/月 总成本=33000+24000=57000CNY/月 一年68.4W

#### 3.2.2.redis容量预估

1. 存在性缓存容量预估
    1. 整体判断下来我们使用布隆过滤器，以用户ID取模为KEY,将存在判断全部添加进去到过滤器中,
       在tair(https://help.aliyun.com/zh/tair/developer-reference/bloom)同时自带扩容能力.
    2. 我们选择100W,0.01区分度,大体2M左右,现状800亿/100W*
       2M=156.25G,未来1160亿/100W=116000个,共226G左右,一般水位在70%到80%是个比较均很水位,同时为未来的布隆过滤器重建有一定的余量,因此整体选择256GTAIR缓存集群版本。
    3. Feed流每次拉取6条数据,每个用户基本落在一个KEY上,所以整体缓存基本在60W左右
    4. 支持异地部署3地,256*3=768G，若打5则，18000*3=54000CNY/月
2. 计数器缓存容量预估
    1. 整体判断下来使用String+布隆过滤器.若基本消费半年数据,一天1000W,大该18亿.
    2. 整体内容有40字节,一个String按照100字节来算，18亿*100/1024/1024/1024=167G
    3. 一个10亿的布隆过滤器2G,用户10亿注册用户基本够了,内容假设库里有100亿数据,未来要支撑200亿计算，大体40G数据,
    4. FEED流每次6条,虽然使用MGET,但到proxy上是会sharding到各个DATA节点去查询的，因此最大整个QPS是41W*
       6=240W,最差布隆过滤器也查询6次,基本也是480WQPS左右,看性能报告,单分片性能基本在30W左右，假设我们有32分片,基本也在960W左右.整体有余量
    5. 总体来看167+40=207G,由于是缓存,容量会时刻变化,因此也选择256GTAIR缓存集群版本。
    6. 支持异地部署3地,256*3=768G，若打5则，18000*3=54000CNY/月
3. 总体价格=54000+54000=10W8,一年费用大概129.6W左右
   总体成本130W+70W=200W左右.

### 3.3.技术决策

1. 主21212

## 4.详细设计

### 4.1.架构总览

``` plantuml
@startuml
!include 关系链路.puml
@enduml
```

1. 整套系统分为关系系统,关系任务系统,关注系统,关系facade系统和关系插件
2. 关系facade是业务的门面,用于对接统一网关,对客户端提供服务。
3. 关系系统是在领域上是业务的底层,所有操作都在这里执行,包含关系写入读取等
4. 关注系统单独抽离出来,但写,存储共用关系系统的底层能力,同时有能力单独读取数据库来做一些自身的业务。
5. 关系插件是一个二方包,用于降低feed系统对系统本身的冲击,直接链接redis,在查询不到的情况通过RPC回流到业务系统是上

### 4.2.核心流程

#### 4.2.1 发布关系

``` plantuml
@startuml
!include 发布关系.puml
@enduml
```

#### 4.2.2.获取关系

``` plantuml
@startuml
!include 获取关系.puml
@enduml
```

#### 4.2.3.更新统计

``` plantuml
@startuml
!include 更新统计.puml
@enduml
```

#### 4.2.4.更新缓存

``` plantuml
@startuml
!include 更新缓存.puml
@enduml
```

1. 212
2.

### 4.3.关键设计

#### 4.3.1.数据库设计

![image.png](https://i.postimg.cc/Jn3gB6SD/image.png)
1. 主表

2. 索引表
   本地二级索引
   全局索引
3. 统计表
    1. 其中objectId是user_id和主表的object_id
    2. type基于二进制自定义,

| 3个字节              | 2个字节 | 2个字节 |
|-------------------|------|------|
| objectId类型(65536) | 预留   | 如闻   | 预留 |

#### 4.3.2.缓存设计

1. 存在性缓存
    1. 选择100W,0.01区分度,大体2M左右,按照key=user_id%100000来做sharding做布隆过滤器,item=userId:objectId:
       objectType+actionType.
    2. 查询带宽统计每次查询8+8+2=18字节,关注/点赞/收藏需要查3个actionType,每次有6条数据,因此, 18*6*
       3=324字节,QPS在41W左右，21W*324/1024=126M带宽,整体还好
    3. 在每隔一年需要开启布隆过滤器重建,重建过程需要扫描数据库按照每个KEY进行重建
2. 统计缓存设计
    1. 缓存为String.key=objectId+类型,value,actiontype+number拼装
    2. 布隆过滤器key为objectId+类型,key=objectId%10000来做sharding,item=objectId:objectType
    3. 在每隔一年需要开启布隆过滤器重建,重建过程需要扫描数据库按照每个KEY进行重建

#### 4.3.3.高可用

##### 4.3.3.1.存储高可用

1. 表格存储数据库本身存在高可用方案,同时我们本身也做了相关多地域的部署
2. 缓存现阶段落盘策略是每秒刷新一次,我们也做了多地域的部署,因此整体比较可靠

##### 4.3.3.2.计算高可用

1. 在单机大于某个数时候会将整体限流,默认返回数字为0,关系表示未存在,用户体验可接受
2. 如果redis如果挂的化,做了相关预案,默认返回数字为0,关系表示未存在,用户体验可接受
3. 在TT消息挂后,可切换成写入数据库同时发RocketMQ,整体链路不变,避免TT的影响

##### 4.2.3.3.数据库&缓存一致

1. 整体方案采用消息驱动,缓存和数据库数据最终一致,而非强一致的保证

#### 4.3.4.高性能

##### 4.3.4.1.存储高性能

1. 数据库和缓存有着极好的弹性,在不足时候可以快速扩容
2. 缓存中采用布隆过滤器这个数据结构来完成在大量数据的情况下完成存在性判断

##### 4.3.4.2.计算高性能

1. 针对大量QPS提出关系数据SDK化,避免将更大的压力传递到应用集群,将redis有限的暴露出去,从而完成性能

#### 4.3.5.可扩展

##### 4.3.5.1 快速支持关系型业务

1. 通过后台申请等方式,申请AK&SK接入关系业务,快速支持存在性,COUNT和列表查询,

##### 4.3.5.2 快速支持统计型业务

1. 统计型业务可以申请AK&SK, 用+-的方式快速进行，同时支持需要对帐和不需要对帐的统计，如播放次数,浏览次数等以及相关播放时长的统计

#### 4.3.6.安全

1. 在一些场景上尤其是关注场景，需要安全规则介入，防止随意刷粉的情况出啊心
2. 21212

#### 4.3.7.其他

### 4.4.部署架构

1. 21212
2. 12212
3. 21212

## 5.横向概念

### 5.1.接口定义

#### 5.1.1.A接口

#### 5.1.2.B接口

### 5.2.中间件配置

#### 5.2.1.动态配置

#### 5.2.2.消息配置

### 5.3.数据及埋点

#### 5.3.1.技术埋点

1. 每种业务类型缓存命中率统计埋点
2. Stringget缓存大小埋点

#### 5.3.2.业务埋点

1. 点赞统计数据埋点

#### 5.3.2.GOC预警

1. 什么指标接入GOC预警

## 6.排期规划



<!-- @include: @article-footer.snippet.md -->
		